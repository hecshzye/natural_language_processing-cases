{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disaster-tweet-detection-nlp.ipynb",
      "provenance": [],
      "mount_file_id": "1C3kARu3gDB2zD9mL4ANBPE6QAKWHuI-e",
      "authorship_tag": "ABX9TyOA1ZndPGx9Tynh1nS7UbQW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hecshzye/natural_language_processing-cases/blob/main/disaster_tweet_detection_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disaster Tweets Detection using Natural Language Processing"
      ],
      "metadata": {
        "id": "8kW8ikcy0G8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The goal is to predict which tweets are about real disasters and which are not using `NLP` and `TensorFlow`\n",
        "\n",
        "- The dataset used in this model is the `Real-or-Not` from `Kaggle competition`: https://www.kaggle.com/c/nlp-getting-started/data\n"
      ],
      "metadata": {
        "id": "D-xaoJM07K-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "x9F73qM63p48"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing few functions wriiten for workflow and ease\n",
        "!wget https://raw.githubusercontent.com/hecshzye/natural_language_processing-cases/main/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhuiKa4Y5nWp",
        "outputId": "3ec3c66b-6210-42fc-d7fd-9039fa5e05bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-14 04:45:09--  https://raw.githubusercontent.com/hecshzye/natural_language_processing-cases/main/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6442 (6.3K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]   6.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-14 04:45:10 (91.4 MB/s) - ‘helper_functions.py’ saved [6442/6442]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves, create_confusion_matrix, create_tensorboard_callback, unzip_data, compare_history"
      ],
      "metadata": {
        "id": "UGhEX4TZ51nT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ly6QBaOM6KGp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}